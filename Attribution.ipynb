{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90eba707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f684e4dc63d44d0aa30d6faa8f97d235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import random\n",
    "import sys\n",
    "model_id = \"gemma-2b-it\"\n",
    "device = \"cuda:0\"\n",
    "using_fine_tuned = False\n",
    "using_instruction_tuned = False\n",
    "using_instruction_tuned_zero = True\n",
    "using_pretrained = False\n",
    "quantized = False\n",
    "from captum.attr import (\n",
    "    FeatureAblation, \n",
    "    ShapleyValues,\n",
    "    LayerIntegratedGradients, \n",
    "    LLMAttribution, \n",
    "    LLMGradientAttribution, \n",
    "    TextTokenInput, \n",
    "    TextTemplateInput,\n",
    "    ProductBaselines,\n",
    "    ShapleyValueSampling,\n",
    "    KernelShap\n",
    ")\n",
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    return bnb_config\n",
    "\n",
    "def load_model(model_name):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = \"24000MB\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=device, # dispatch efficiently the model on the available ressources\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.float16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "if quantized:\n",
    "    bnb_config = create_bnb_config()\n",
    "    model, tokenizer = load_model(model_id)\n",
    "else:\n",
    "    max_memory = \"24000MB\"\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id,device_map=device,\n",
    "                    torch_dtype=torch.float16,trust_remote_code=True, attn_implementation=\"flash_attention_2\",\n",
    "                    max_memory = {i: max_memory for i in range(n_gpus)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e530319b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9097, 32970, 15679,  7286], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lm_saliency import saliency, input_x_gradient, erasure_scores, l1_grad_norm, l2_grad_norm\n",
    "from datasets import load_from_disk\n",
    "world_id = tokenizer.convert_tokens_to_ids(\"world\")\n",
    "business_id = tokenizer.convert_tokens_to_ids(\"business\")\n",
    "sport_id = tokenizer.convert_tokens_to_ids(\"sport\")\n",
    "tech_id = tokenizer.convert_tokens_to_ids(\"tech\")\n",
    "positive_id = tokenizer.convert_tokens_to_ids(\"positive\")\n",
    "negative_id = tokenizer.convert_tokens_to_ids(\"negative\")\n",
    "wsbt = torch.LongTensor([world_id, sport_id, business_id, tech_id ]).to(device)\n",
    "np_ = torch.LongTensor([negative_id, positive_id]).to(device)\n",
    "news = load_from_disk(\"news\")\n",
    "imdb = load_from_disk(\"imdb\")\n",
    "sst2 = load_from_disk(\"sst2\")\n",
    "news_shot = news[\"train\"][10][\"text\"]\n",
    "imdb_shots = imdb[\"train\"].filter(lambda x:len(tokenizer(x[\"text\"])[\"input_ids\"]) < 32)\n",
    "sst_shots = sst2[\"train\"].filter(lambda x:len(tokenizer(x[\"sentence\"])[\"input_ids\"]) < 16)\n",
    "news_shots = news[\"train\"].filter(lambda x:len(tokenizer(x[\"text\"])[\"input_ids\"]) < 32)\n",
    "news = news[\"test\"].select(range(4000)).filter(lambda x:len(tokenizer(x[\"text\"])[\"input_ids\"]) <= 48)\n",
    "news = news.shuffle(2024)\n",
    "num_samples = 200\n",
    "news = news.select(range(num_samples))\n",
    "imdb = imdb[\"test\"].filter(lambda x:len(tokenizer(x[\"text\"])[\"input_ids\"]) <= 96)\n",
    "imdb = imdb.shuffle(2024)\n",
    "imdb = imdb.select(range(num_samples))\n",
    "sst2 = sst2[\"validation\"].filter(lambda x:len(tokenizer(x[\"sentence\"])[\"input_ids\"]) <= 32)\n",
    "sst2 = sst2.shuffle(2024)\n",
    "sst2 = sst2.select(range(num_samples))\n",
    "val_data = {}\n",
    "val_data[\"sst2\"] = sst2\n",
    "val_data[\"imdb\"] = imdb\n",
    "val_data[\"news\"] = news\n",
    "wsbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb20a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def captum_attrs(model,tokenizer):\n",
    "    lig = LayerIntegratedGradients(model,model.model.embed_tokens)\n",
    "    lig_steps = 5\n",
    "    sv = ShapleyValues(model)\n",
    "    ks = KernelShap(model)\n",
    "\n",
    "    lig_attr = LLMGradientAttribution(lig, tokenizer)\n",
    "    sv_attr = LLMAttribution(sv, tokenizer)\n",
    "    kernel_attr = LLMAttribution(ks, tokenizer)\n",
    "    return lig_attr, sv_attr, kernel_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f1de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f66e344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [10:53<00:00,  3.27s/it]\n",
      "100%|██████████| 200/200 [09:35<00:00,  2.88s/it]\n",
      "100%|██████████| 200/200 [05:50<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "datasets = [\"imdb\",\"news\",\"sst2\"]\n",
    "\n",
    "for ds in datasets:\n",
    "    if using_fine_tuned:\n",
    "        if quantized:\n",
    "            model, _ = load_model(f\"predictor_{model_id}_{ds}_merged\")\n",
    "        else:\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(f\"predictor_{model_id}_{ds}_merged\",device_map=\"auto\",\n",
    "                    torch_dtype=torch.float16,trust_remote_code=True, attn_implementation=\"flash_attention_2\",\n",
    "                    max_memory = {i: max_memory for i in range(n_gpus)})\n",
    "\n",
    "    explanations_dict = defaultdict(list)\n",
    "\n",
    "    lig_attr, sv_attr, kernel_attr = captum_attrs(model,tokenizer)\n",
    "    lig_steps = 5\n",
    "    for i in tqdm(val_data[ds]):\n",
    "        if ds ==\"sst2\":\n",
    "            comment = i[\"sentence\"]\n",
    "        else:\n",
    "            comment = i[\"text\"]\n",
    "\n",
    "        if ds == \"sst2\" or ds==\"imdb\":\n",
    "            model.zero_grad()\n",
    "            torch.cuda.empty_cache()\n",
    "            if using_fine_tuned:\n",
    "                \n",
    "                if ds == \"sst2\":\n",
    "                    eval_prompt = comment + \" In a sentiment classification task between positive and negative choices, the sentiment of this sentence is \"\n",
    "                elif ds == \"imdb\":\n",
    "                    eval_prompt = comment + \" Based on this opinion, decide what the sentiment is, choose between positive and negative. Answer is \"\n",
    "            \n",
    "            elif using_instruction_tuned:\n",
    "                if ds == \"imdb\":\n",
    "                    eval_prompt = f\"\"\"In the sentence in triple back ticks what is the sentiment? Answer in one word, positive or negative.\n",
    "                    example 1: ```{imdb_shots[15][\"text\"]}```\n",
    "                    The answer is positive\n",
    "                    example 2: ```{comment}```\n",
    "                    The answer is \"\"\"\n",
    "                elif ds == \"sst2\":\n",
    "                    eval_prompt = f\"\"\"In the sentence in triple back ticks what is the sentiment? Answer in one word, positive or negative.\n",
    "                    example 1: ```{sst_shots[18][\"sentence\"]}```\n",
    "                    The answer is negative\n",
    "                    example 2: ```{comment}```\n",
    "                    The answer is \"\"\"\n",
    "            elif using_instruction_tuned_zero:\n",
    "                if ds==\"imdb\":\n",
    "                    eval_prompt = f\"\"\"In the sentence in triple back ticks what is the sentiment? Answer in one word, positive or negative.\n",
    "```{comment}```\n",
    "The answer is \"\"\"\n",
    "                elif ds == \"sst2\":\n",
    "                    eval_prompt = f\"\"\"In the sentence in triple back ticks what is the sentiment? Answer in one word, positive or negative.\n",
    "```{comment}```\n",
    "The answer is \"\"\"\n",
    "            model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
    "            out = model(**model_input)\n",
    "            out_id = torch.argmax(out.logits[0,-1][np_])\n",
    "            CORRECT_ID = np_[torch.argmax(out.logits[0,-1][np_])]\n",
    "            FOIL_ID = np_[torch.argsort(out.logits[0,-1][np_])[-2]]\n",
    "            explanations_dict[\"comment\"].append(comment)\n",
    "            explanations_dict[\"prompt\"].append(eval_prompt)\n",
    "            explanations_dict[\"gold_id\"].append(np_[i[\"label\"]].cpu().numpy())\n",
    "            explanations_dict[\"predicted_id\"].append(CORRECT_ID.cpu().numpy())\n",
    "            explanations_dict[\"foil_id\"].append(FOIL_ID.cpu().numpy())\n",
    "\n",
    "            saliency_matrix, embd_matrix = saliency(model, model_input[\"input_ids\"].squeeze(0), model_input[\"attention_mask\"].squeeze(0),\n",
    "                                     correct=CORRECT_ID, foil=FOIL_ID)\n",
    "\n",
    "            model.zero_grad()\n",
    "            contra_explanation = l1_grad_norm(saliency_matrix, normalize=False)\n",
    "            explanations_dict['gradnorm1'].append(contra_explanation)\n",
    "            contra_explanation = l2_grad_norm(saliency_matrix, normalize=False)\n",
    "            explanations_dict['gradnorm2'].append(contra_explanation)\n",
    "            contra_explanation = input_x_gradient(saliency_matrix, embd_matrix, normalize=False)\n",
    "            explanations_dict['gradinp'].append(contra_explanation)\n",
    "            contra_explanation = erasure_scores(model, model_input[\"input_ids\"].squeeze(0), model_input[\"attention_mask\"].squeeze(0),\n",
    "                                      correct=CORRECT_ID, foil=FOIL_ID, normalize=False)\n",
    "            explanations_dict['erasure'].append(contra_explanation)\n",
    "\n",
    "            target_c = tokenizer.convert_ids_to_tokens(CORRECT_ID.cpu().tolist())\n",
    "            target_f = tokenizer.convert_ids_to_tokens(FOIL_ID.cpu().tolist())\n",
    "\n",
    "            inp = TextTokenInput(eval_prompt, tokenizer, skip_tokens=[0])\n",
    "            \n",
    "            attr_res_c = lig_attr.attribute(inp, target=target_c,n_steps=lig_steps)\n",
    "            attr_res_f = lig_attr.attribute(inp, target=target_f,n_steps=lig_steps)\n",
    "\n",
    "            contra_explanation = (attr_res_c.seq_attr/torch.norm(attr_res_c.seq_attr) -\n",
    "                attr_res_f.seq_attr/torch.norm(attr_res_f.seq_attr)).cpu().numpy()\n",
    "            explanations_dict['integrated_grad'].append(contra_explanation)\n",
    "\n",
    "            attr_res_c = kernel_attr.attribute(inp, target=target_c)\n",
    "            attr_res_f = kernel_attr.attribute(inp, target=target_f)\n",
    "            contra_explanation = (attr_res_c.seq_attr/torch.norm(attr_res_c.seq_attr) -\n",
    "                attr_res_f.seq_attr/torch.norm(attr_res_f.seq_attr)).cpu().numpy()\n",
    "            explanations_dict['kernel_shap'].append(contra_explanation)\n",
    "\n",
    "\n",
    "            contra_explanation = np.random.normal(0,1,model_input[\"input_ids\"].squeeze(0).shape[0])\n",
    "            explanations_dict['random'].append(contra_explanation)\n",
    "\n",
    "\n",
    "        elif ds ==\"news\":\n",
    "            model.zero_grad()\n",
    "            torch.cuda.empty_cache()\n",
    "            if using_fine_tuned:\n",
    "\n",
    "                eval_prompt = comment + \" You are classifying a news article, Choose one of the four categories, World, Business, Sport, and Tech. Answer is \"\n",
    "                \n",
    "            elif using_instruction_tuned:\n",
    "                eval_prompt = f\"\"\"In the sentence in triple back ticks, what is the news category?\\n \\\n",
    "                  choices are world, sports, business, and tech. \\nanswer in one word.\\n world, or sports, or business, or tech.\n",
    "                    example 1: ```{news_shot}```\n",
    "                    The answer is business\n",
    "                    example 2: ```{comment}```\n",
    "                    The answer is\"\"\"\n",
    "                \n",
    "            elif using_instruction_tuned_zero:\n",
    "                eval_prompt = f\"\"\"In the sentence in triple back ticks, what is the news category?\n",
    "                  choices are world, sports, business, and tech. answer in one word.\n",
    "```{comment}```\n",
    "The answer is \"\"\"\n",
    "            model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
    "            out = model(**model_input)\n",
    "            out_id = torch.argmax(out.logits[0,-1][wsbt])\n",
    "            CORRECT_ID = wsbt[torch.argmax(out.logits[0,-1][wsbt])]\n",
    "            FOIL_ID = wsbt[torch.argsort(out.logits[0,-1][wsbt])[-2]]\n",
    "            explanations_dict[\"comment\"].append(comment)\n",
    "            explanations_dict[\"prompt\"].append(eval_prompt)\n",
    "            explanations_dict[\"gold_id\"].append(wsbt[i[\"label\"]].cpu().numpy())\n",
    "            explanations_dict[\"predicted_id\"].append(CORRECT_ID.cpu().numpy())\n",
    "            explanations_dict[\"foil_id\"].append(FOIL_ID.cpu().numpy())\n",
    "            \n",
    "            saliency_matrix, embd_matrix = saliency(model, model_input[\"input_ids\"].squeeze(0), model_input[\"attention_mask\"].squeeze(0),\n",
    "                                     correct=CORRECT_ID, foil=FOIL_ID)\n",
    "\n",
    "            model.zero_grad()\n",
    "            contra_explanation = l1_grad_norm(saliency_matrix, normalize=False)\n",
    "            \n",
    "            explanations_dict['gradnorm1'].append(contra_explanation)\n",
    "            contra_explanation = l2_grad_norm(saliency_matrix, normalize=False)\n",
    "            explanations_dict['gradnorm2'].append(contra_explanation)\n",
    "            contra_explanation = input_x_gradient(saliency_matrix, embd_matrix, normalize=False)\n",
    "            explanations_dict['gradinp'].append(contra_explanation)\n",
    "            contra_explanation = erasure_scores(model, model_input[\"input_ids\"].squeeze(0), model_input[\"attention_mask\"].squeeze(0),\n",
    "                                      correct=CORRECT_ID, foil=FOIL_ID, normalize=False)\n",
    "            explanations_dict['erasure'].append(contra_explanation)\n",
    "            model.zero_grad()\n",
    "            torch.cuda.empty_cache()\n",
    "            target_c = tokenizer.convert_ids_to_tokens(CORRECT_ID.cpu().tolist())\n",
    "            target_f = tokenizer.convert_ids_to_tokens(FOIL_ID.cpu().tolist())\n",
    "            inp = TextTokenInput(\n",
    "                eval_prompt, \n",
    "                tokenizer,\n",
    "                skip_tokens=[0], \n",
    "            )\n",
    "            attr_res_c = lig_attr.attribute(inp, target=target_c,n_steps=lig_steps)\n",
    "            attr_res_f = lig_attr.attribute(inp, target=target_f,n_steps=lig_steps)\n",
    "\n",
    "            contra_explanation = (attr_res_c.seq_attr/torch.norm(attr_res_c.seq_attr) -\n",
    "                attr_res_f.seq_attr/torch.norm(attr_res_f.seq_attr)).cpu().numpy()\n",
    "            explanations_dict['integrated_grad'].append(contra_explanation)\n",
    "\n",
    "            attr_res_c = kernel_attr.attribute(inp, target=target_c)\n",
    "            attr_res_f = kernel_attr.attribute(inp, target=target_f)\n",
    "\n",
    "            contra_explanation = (attr_res_c.seq_attr/torch.norm(attr_res_c.seq_attr) -\n",
    "                attr_res_f.seq_attr/torch.norm(attr_res_f.seq_attr)).cpu().numpy()\n",
    "            explanations_dict['kernel_shap'].append(contra_explanation)\n",
    "\n",
    "            contra_explanation = np.random.normal(0,1,model_input[\"input_ids\"].squeeze(0).shape[0])\n",
    "            explanations_dict['random'].append(contra_explanation)\n",
    "\n",
    "    with open(f\"attrs/{ds}_{model_id}_attr_quantized_{quantized}_zero.pcl\",\"bw\") as f:\n",
    "        pickle.dump(explanations_dict,f)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
