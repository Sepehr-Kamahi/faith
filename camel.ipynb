{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import bitsandbytes as bnb\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","import random\n","import sys\n","\n","using_fine_tuned = False\n","\n","\n","def create_bnb_config():\n","    bnb_config = BitsAndBytesConfig(\n","        load_in_8bit=True,\n","        llm_int8_threshold=4.0,\n","\n","    )\n","    return bnb_config\n","\n","def load_model(model_name,device):\n","    n_gpus = torch.cuda.device_count()\n","    max_memory = \"24000MB\"\n","\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_name,\n","        device_map=device, # dispatch efficiently the model on the available ressources\n","        quantization_config=bnb_config,\n","        torch_dtype=torch.float16,\n","        trust_remote_code = True,\n","        attn_implementation=\"flash_attention_2\",\n","        max_memory = {i: max_memory for i in range(n_gpus)},\n","    )\n","\n","    return model\n","bnb_config = create_bnb_config()\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T03:13:15.598102Z","iopub.status.busy":"2024-05-18T03:13:15.597551Z","iopub.status.idle":"2024-05-18T03:13:16.151028Z","shell.execute_reply":"2024-05-18T03:13:16.150286Z","shell.execute_reply.started":"2024-05-18T03:13:15.598061Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/sepeka/miniconda3/envs/faith11/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]}],"source":["from transformers import AutoTokenizer,AutoModelForCausalLM\n","import torch\n","cfg_device = \"cuda:0\"\n","pred_device = \"cuda:0\"\n","cfg_model_id = \"gptj\"\n","\n","if  cfg_model_id ==\"phi3\":\n","    cfg_model = AutoModelForCausalLM.from_pretrained(\n","    cfg_model_id,device_map = cfg_device, torch_dtype=torch.bfloat16,trust_remote_code = True, attn_implementation=\"flash_attention_2\")\n","    cfg_tokenizer =  AutoTokenizer.from_pretrained(cfg_model_id,trust_remote_code = True)\n","\n","elif cfg_model_id ==\"phi3-medium\":\n","    \n","    cfg_model = load_model(cfg_model_id,cfg_device)\n","    cfg_tokenizer =  AutoTokenizer.from_pretrained(cfg_model_id,trust_remote_code = True)\n","\n","\n","else:\n","    if cfg_model_id == \"pythia\":\n","        cfg_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-1.4b\")\n","        cfg_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-1.4b\",\n","                                                         device_map = cfg_device,\n","                                                         torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\")\n","    elif cfg_model_id == \"pythia2\":\n","        cfg_tokenizer = AutoTokenizer.from_pretrained(\"pythia-2.8b\")\n","        cfg_model = AutoModelForCausalLM.from_pretrained(\"pythia-2.8b\",\n","                                                         device_map = cfg_device,\n","                                                         torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\")\n","    \n","    elif cfg_model_id == \"gptj\":\n","        cfg_model = AutoModelForCausalLM.from_pretrained(\n","        \"EleutherAI/gpt-j-6b\",device_map = cfg_device, torch_dtype=torch.bfloat16,trust_remote_code = True)\n","        cfg_tokenizer = AutoTokenizer.from_pretrained(\n","        \"EleutherAI/gpt-j-6b\",)\n","\n","if cfg_model_id == \"phi3-medium\":\n","    pass\n","else:\n","    num_added_toks = cfg_tokenizer.add_tokens([\"<mask>\", \"<counterfactual>\"])\n","    cfg_model.resize_token_embeddings(len(cfg_tokenizer))\n","# pred_model = AutoModelForCausalLM.from_pretrained(\"gemma-2b-it\",device_map=pred_device,\n","#                         torch_dtype=torch.float16,trust_remote_code=True, attn_implementation=\"flash_attention_2\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T03:13:17.016484Z","iopub.status.busy":"2024-05-18T03:13:17.015748Z","iopub.status.idle":"2024-05-18T03:13:17.046078Z","shell.execute_reply":"2024-05-18T03:13:17.045182Z","shell.execute_reply.started":"2024-05-18T03:13:17.016451Z"},"trusted":true},"outputs":[],"source":["import pickle\n","import pandas as pd\n","from collections import defaultdict\n","from tqdm import tqdm\n","attrs = {}\n","datasets = [\"sst2\",\"news\",\"imdb\"]\n","models = [\"gemma-2b\"]\n","zero = False\n","\n","for ds in datasets:\n","    if zero == True:\n","        att_name = f\"attrs/{ds}_gemma-2b-it_attr_quantized_False_zero.pcl\"\n","        with open(att_name,\"br\") as f:\n","            attr = pickle.load(f)\n","            attr_df = pd.DataFrame(attr)\n","            attrs[f\"{ds}_gemma-2b-it-zero\"] = attr_df\n","    else:\n","        for model in models:\n","\n","            att_name = f\"attrs/{ds}_{model}-it_attr_quantized_False.pcl\"\n","            with open(att_name,\"br\") as f:\n","                attr = pickle.load(f)\n","                attr_df = pd.DataFrame(attr)\n","                attrs[f\"{ds}_{model}-it\"] = attr_df\n","\n","            att_name = f\"attrs/{ds}_predictor_{model}_{ds}_merged_attr_quantized_False.pcl\"\n","            with open(att_name,\"br\") as f:\n","                attr = pickle.load(f)\n","                attr_df = pd.DataFrame(attr)\n","                attrs[f\"{ds}_{model}-ft\"] = attr_df\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['sst2_gemma-2b-it', 'sst2_gemma-2b-ft', 'news_gemma-2b-it', 'news_gemma-2b-ft', 'imdb_gemma-2b-it', 'imdb_gemma-2b-ft'])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["attrs.keys()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from datasets import load_from_disk\n","if zero:\n","    pass\n","else:\n","    news = load_from_disk(\"news\")\n","    imdb = load_from_disk(\"imdb\")\n","    sst2 = load_from_disk(\"sst2\")\n","    news_shot = news[\"train\"][10][\"text\"]\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T03:16:24.326249Z","iopub.status.busy":"2024-05-18T03:16:24.325689Z","iopub.status.idle":"2024-05-18T03:16:24.406156Z","shell.execute_reply":"2024-05-18T03:16:24.405252Z","shell.execute_reply.started":"2024-05-18T03:16:24.326214Z"},"trusted":true},"outputs":[],"source":["import torch.nn.functional as F\n","def log_probs_from_logits(logits, labels):\n","    logp = F.log_softmax(logits, dim=-1)\n","    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n","    return logp_label\n","def sequence_logprob(model, labels, input_len=0):\n","    with torch.no_grad():\n","        output = model(labels)\n","        log_probs = log_probs_from_logits(\n","        output.logits[:, :-1, :], labels[:, 1:])\n","        seq_log_prob = torch.sum(log_probs[:, input_len:])\n","        return seq_log_prob.float().cpu().numpy()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T03:16:25.445652Z","iopub.status.busy":"2024-05-18T03:16:25.445273Z","iopub.status.idle":"2024-05-18T03:16:25.453011Z","shell.execute_reply":"2024-05-18T03:16:25.452106Z","shell.execute_reply.started":"2024-05-18T03:16:25.445623Z"},"trusted":true},"outputs":[],"source":["def get_completion(prompt,ds):\n","    \n","    inputs = cfg_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(cfg_device)\n","    attention_mask = cfg_tokenizer(prompt, return_tensors=\"pt\").attention_mask.to(cfg_device)\n","\n","    sen_len = len(inputs[0])\n","\n","\n","    # output_beam = cfg_model.generate(inputs,attention_mask = attention_mask, max_length=(3*inputs.shape[1])-1, num_beams=5, top_p=0.95,\n","    # do_sample=True, temperature=.5)\n","    output_beam = cfg_model.generate(inputs,attention_mask = attention_mask, max_new_tokens=sen_len-1, num_beams=3, \n","        #top_p=0.95, do_sample=True, temperature=.5\n","        )\n","    if ds == \"sst2\"or ds == \"imdb\":\n","        return cfg_tokenizer.decode(output_beam[0][sen_len+1:-1])\n","    else:\n","        return cfg_tokenizer.decode(output_beam[0][sen_len+1:-3])\n","    \n","def get_completion_instruct(sentence,ds,foil_sentiment):\n","    sen_new = sentence.replace(\"<mask>\",\"<unk>\")\n","    sen_len = len(cfg_tokenizer(sen_new).input_ids)\n","\n","    if ds == \"sst2\" or ds == \"imdb\":\n","        prompt = f\"\"\" In the statement in backticks, replace any <unk> with a word in a way \\\n","that the resulting statement would have a {foil_sentiment} sentiment.\n","output just the completed sentence.\n","```{sen_new}```\n","Answer:\n","\"\"\"\n","        inputs = cfg_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(cfg_device)\n","        attention_mask = cfg_tokenizer(prompt, return_tensors=\"pt\").attention_mask.to(cfg_device)\n","\n","        output_beam = cfg_model.generate(inputs,attention_mask = attention_mask, max_new_tokens=sen_len+1, num_beams=3,\n","        #top_p=0.95, do_sample=True, temperature=.5\n","        )\n","        return  prompt, cfg_tokenizer.decode(output_beam[0][inputs.shape[1]:])\n","    else:\n","        prompt = f\"\"\" In the article in backticks, replace any <unk> with a word in a way \\\n","that the resulting article would have a {foil_sentiment} category.\n","output just the completed sentence.\n","```{sen_new}```\n","Answer:\n","\"\"\"\n","        inputs = cfg_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(cfg_device)\n","\n","        attention_mask = cfg_tokenizer(prompt, return_tensors=\"pt\").attention_mask.to(cfg_device)\n","        output_beam = cfg_model.generate(inputs,attention_mask = attention_mask, max_new_tokens=sen_len+1, num_beams=3, \n","        #top_p=0.95, do_sample=True, temperature=.5\n","        )\n","        return prompt, cfg_tokenizer.decode(output_beam[0][inputs.shape[1]:])"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","from transformers import logging\n","logging.set_verbosity(logging.ERROR)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T05:39:57.670004Z","iopub.status.busy":"2024-05-18T05:39:57.669237Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2d1a837e843406c81faa53357348e20","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 200/200 [1:05:00<00:00, 19.50s/it]\n"]}],"source":["import numpy as np\n","using_fine_tuned = [False]\n","percentages = [0.1,0.2,0.3,0.4,0.5]\n","nums = [0,200]\n","datasets = [\"sst2\"]\n","pred_models = [\"gemma-2b\"]\n","for ds in datasets:\n","    if cfg_model_id == \"phi3-medium\":\n","        pass\n","    else: \n","        cfg_ds_specific = f\"counter_{cfg_model_id}_{ds}_finished.pt\"\n","        cfg_model.load_state_dict(torch.load(cfg_ds_specific,map_location=cfg_device))\n","\n","\n","    torch.cuda.empty_cache()\n","    for pr_model in pred_models:\n","        \n","        for ft in using_fine_tuned:\n","            flipping_dict = defaultdict(list)\n","\n","\n","            if ft:\n","                pred_model_id = f\"predictor_{pr_model}_{ds}_merged\"\n","                eval_data = attrs[f'{ds}_{pr_model}-ft']\n","            else:\n","                \n","                if zero:\n","                    pred_model_id = f\"{pr_model}-it\"\n","                    eval_data = attrs[f'{ds}_{pr_model}-it-zero']\n","                else:\n","                    pred_model_id = f\"{pr_model}-it\"\n","                    eval_data = attrs[f'{ds}_{pr_model}-it']\n","\n","\n","            pred_model = AutoModelForCausalLM.from_pretrained(pred_model_id, device_map=pred_device,\n","                        torch_dtype=torch.float16,trust_remote_code=True, attn_implementation=\"flash_attention_2\")\n","            pred_tokenizer = AutoTokenizer.from_pretrained(pred_model_id)\n","            if zero == False:\n","                imdb_shots = imdb[\"train\"].filter(lambda x:len(pred_tokenizer(x[\"text\"])[\"input_ids\"]) < 32)\n","                sst_shots = sst2[\"train\"].filter(lambda x:len(pred_tokenizer(x[\"sentence\"])[\"input_ids\"]) < 16)\n","            \n","            for i in tqdm(range(nums[0],nums[1])):\n","\n","                flipping_dict[\"sentence\"].append(eval_data[\"comment\"].iloc[i])\n","                explanation_types = list(eval_data.columns[5:])\n","                for exp in explanation_types:\n","                    flipped = False\n","                    attr = eval_data[exp].iloc[i]\n","\n","\n","                    for percent in percentages:\n","                        num_to_mask = int(np.ceil(percent * len(pred_tokenizer(eval_data.iloc[i].comment)[\"input_ids\"])))\n","\n","                        idx_to_mask = np.argsort(attr)[-int(num_to_mask):]\n","\n","                        tokens = pred_tokenizer.convert_ids_to_tokens(pred_tokenizer(eval_data.prompt.iloc[i])[\"input_ids\"])\n","\n","                        if ft:\n","                            for idss in idx_to_mask:\n","                                tokens[idss] = \"<mask>\"\n","\n","                            if ds ==\"sst2\":\n","                                sentence = pred_tokenizer.convert_tokens_to_string(tokens[1:-18])\n","                            if ds ==\"news\":\n","                                sentence = pred_tokenizer.convert_tokens_to_string(tokens[1:-26])\n","                            if ds ==\"imdb\":\n","                                sentence = pred_tokenizer.convert_tokens_to_string(tokens[1:-20])\n","                        else:\n","                            if zero:\n","                                \n","                                for idss in idx_to_mask:\n","                                    tokens[idss] = \"<mask>\"\n","                                \n","\n","                                if ds ==\"sst2\":\n","                                    sentence = pred_tokenizer.convert_tokens_to_string(tokens[24:-6])\n","                                if ds ==\"news\":\n","                                    sentence = pred_tokenizer.convert_tokens_to_string(tokens[35:-6])\n","                                if ds ==\"imdb\":\n","                                    sentence = pred_tokenizer.convert_tokens_to_string(tokens[24:-6])\n","                                \n","                            else:\n","                                for idss in idx_to_mask:\n","                                    tokens[idss] = \"<mask>\"\n","\n","                                if ds ==\"sst2\":\n","                                    sentence = pred_tokenizer.convert_tokens_to_string(tokens[54:-7])\n","                                if ds ==\"news\":\n","                                    sentence = pred_tokenizer.convert_tokens_to_string(tokens[118:-6])\n","                                if ds ==\"imdb\":\n","                                    sentence = pred_tokenizer.convert_tokens_to_string(tokens[72:-7])\n","\n","\n","                        flipping_dict[f\"{exp}_masked_sentence_{percent}_predictor_{pr_model}_ft_{ft}\"].append(sentence)\n","                        predicted_label = eval_data.predicted_id.iloc[i]\n","\n","                        foil_id = eval_data.foil_id.iloc[i].tolist()\n","\n","                        foil_sentiment = pred_tokenizer.convert_ids_to_tokens(foil_id)\n","\n","                        \n","                        if cfg_model_id == \"phi3-medium\":\n","\n","                            prompt, response = get_completion_instruct(sentence,ds,foil_sentiment)\n","\n","\n","                        else:\n","                            \n","                            prompt = f\"{sentence}{foil_sentiment}<counterfactual> \"\n","                            response = get_completion(prompt,ds)\n","\n","\n","\n","\n","                        flipping_dict[f\"{exp}_cfg_{cfg_model_id}_{percent}_response_predictor_{pr_model}_ft_{ft}\"].append(response)\n","                        flipping_dict[f\"{exp}_cfg_{cfg_model_id}_{percent}_prompt_predictor_{pr_model}_ft_{ft}\"].append(prompt)\n","\n","                        if ft:\n","                \n","                            if ds == \"sst2\":\n","                                eval_prompt = response + \" In a sentiment classification task between positive and negative choices, the sentiment of this sentence is \"\n","                            elif ds == \"imdb\":\n","                                eval_prompt = response + \" Based on this opinion, decide what the sentiment is, choose between positive and negative. Answer is \"\n","                            elif ds == \"news\":\n","                                eval_prompt = response + \" You are classifying a news article, Choose one of the four categories, World, Business, Sport, and Tech. Answer is \"\n","\n","                        else:\n","                            if zero:\n","                                if ds==\"imdb\":\n","                                    eval_prompt = f\"\"\"In the sentence in triple back ticks what is the sentiment? Answer in one word, positive or negative.\n","```{response}```\n","The answer is \"\"\"\n","                                elif ds == \"sst2\":\n","                                    eval_prompt = f\"\"\"In the sentence in triple back ticks what is the sentiment? Answer in one word, positive or negative.\n","```{response}```\n","The answer is \"\"\"\n","                                elif ds == \"news\":\n","                                    eval_prompt = f\"\"\"In the sentence in triple back ticks, what is the news category?\n","                  choices are world, sports, business, and tech. answer in one word.\n","```{response}```\n","The answer is \"\"\"\n","                            else:\n","                                if ds == \"imdb\":\n","                                    eval_prompt = f\"\"\"In the sentence in triple back ticks what is the sentiment? Answer in one word, positive or negative.\n","                                    example 1: ```{imdb_shots[15][\"text\"]}```\n","                                    The answer is positive\n","                                    example 2: ```{response}```\n","                                    The answer is \"\"\"\n","                                elif ds == \"sst2\":\n","                                    eval_prompt = f\"\"\"In the sentence in triple back ticks what is the sentiment? Answer in one word, positive or negative.\n","                                    example 1: ```{sst_shots[18][\"sentence\"]}```\n","                                    The answer is negative\n","                                    example 2: ```{response}```\n","                                    The answer is \"\"\"\n","                                elif ds == \"news\":\n","                                    eval_prompt = f\"\"\"In the sentence in triple back ticks, what is the news category?\\n \\\n","                                        choices are world, sports, business, and tech. \\nanswer in one word.\\n world, or sports, or business, or tech.\n","                                            example 1: ```{news_shot}```\n","                                            The answer is business\n","                                            example 2: ```{response}```\n","                                            The answer is\"\"\"\n","                        \n","                        encoding = pred_tokenizer(eval_prompt, return_tensors=\"pt\")\n","                        flipping_dict[f\"{exp}_cfg_{cfg_model_id}_{percent}_evalprompt_predictor_{pr_model}_ft_{ft}\"].append(eval_prompt)\n","                        ids = encoding.input_ids.to(pred_device)\n","                        mask = encoding.attention_mask.to(pred_device)\n","                        logits  = pred_model(input_ids = ids, attention_mask = mask).logits[0,-1]\n","                        if logits[foil_id] > logits[predicted_label]:\n","                            if flipped == False:\n","                                flipped = True\n","                                mask_percent = percent\n","\n","                        if flipped == False and percent == 0.5:\n","                            mask_percent = percent\n","                            \n","                    flipping_dict[f\"{exp}_mask_percent_predictor_{pr_model}_ft_{ft}\"].append(mask_percent)\n","                    flipping_dict[f\"{exp}_succuss_in_flipping_predictor_{pr_model}_ft_{ft}\"].append(flipped)\n","\n","            if zero:\n","                with open(f\"camel/cfg_{cfg_model_id}_{ds}_predictor_{pr_model}_ft_{ft}_{nums[0]}_{nums[1]}_zero.pcl\",\"+bw\") as f:\n","                    pickle.dump(flipping_dict,f)\n","            else:\n","                with open(f\"camel/cfg_{cfg_model_id}_{ds}_predictor_{pr_model}_ft_{ft}_{nums[0]}_{nums[1]}.pcl\",\"+bw\") as f:\n","                    pickle.dump(flipping_dict,f)\n"," \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5001540,"sourceId":8405147,"sourceType":"datasetVersion"},{"datasetId":5009766,"sourceId":8416387,"sourceType":"datasetVersion"},{"datasetId":5017138,"sourceId":8425925,"sourceType":"datasetVersion"},{"datasetId":5020535,"sourceId":8430495,"sourceType":"datasetVersion"},{"datasetId":5027041,"sourceId":8439134,"sourceType":"datasetVersion"},{"datasetId":5027279,"sourceId":8439437,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
