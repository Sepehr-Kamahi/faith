{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be88d30f5ca415293e6615f82d2d983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import set_seed\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import os\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "model_id = \"gemma-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model_base = AutoModelForCausalLM.from_pretrained(model_id,torch_dtype = torch.bfloat16,device_map = device)\n",
    "import numpy as np\n",
    "\n",
    "dataset = \"sst2\"\n",
    "dataset_org = load_from_disk(dataset)\n",
    "\n",
    "yes_id = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "no_id = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "shuffled_data = dataset_org.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 233,177,088 || all params: 8,770,857,984 || trainable%: 2.658543650180712\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "if \"gemma\" in model_id:\n",
    "    target_modules = [\"embed_tokens\",\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\",\"lm_head\"]\n",
    "    if model_id == \"gemma-7b\":\n",
    "        r, alpha = 64, 128\n",
    "    elif model_id == \"gemma-2b\":\n",
    "        r, alpha = 512, 1024\n",
    "    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "                             target_modules = target_modules, r=r, lora_alpha=alpha, lora_dropout=0.1)\n",
    "model = get_peft_model(model_base, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if dataset == \"imdb\" or  dataset == \"news\":\n",
    "    shuffled_data[\"test\"] = shuffled_data[\"test\"].select(range(3000))\n",
    "elif dataset == \"sst2\":\n",
    "    shuffled_data[\"test\"] = shuffled_data[\"validation\"]\n",
    "\n",
    "if dataset == \"imdb\" or  dataset == \"sst2\":\n",
    "    shuffled_data = shuffled_data.filter(lambda x: x[\"label\"] in [0,1])\n",
    "elif dataset == \"news\":\n",
    "    shuffled_data = shuffled_data.filter(lambda x: x[\"label\"] in [0,1,2,3])\n",
    "if dataset == \"imdb\" or  dataset == \"news\":\n",
    "    shuffled_data = shuffled_data.filter(lambda x: isinstance(x[\"text\"], str))\n",
    "elif dataset == \"sst2\":\n",
    "    shuffled_data = shuffled_data.filter(lambda x: isinstance(x[\"sentence\"], str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9254, 25832, 14103, 10772, 30212, 31827)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world_id = tokenizer.convert_tokens_to_ids(\"World\")\n",
    "business_id = tokenizer.convert_tokens_to_ids(\"Business\")\n",
    "sport_id = tokenizer.convert_tokens_to_ids(\"Sport\")\n",
    "tech_id = tokenizer.convert_tokens_to_ids(\"Tech\")\n",
    "positive_id = tokenizer.convert_tokens_to_ids(\"positive\")\n",
    "negative_id = tokenizer.convert_tokens_to_ids(\"negative\")\n",
    "tech_id, sport_id, business_id, world_id, positive_id, negative_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df8bf28d29c45a19eafa989cc2f947d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0212d49d09d34a20af0ffdef8a1307e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd608253b9a4d10aee7f9d3db2a1937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29510c8ed26492fb3a0288c3e9dc7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def instruction(batch):\n",
    "    if dataset == \"news\":\n",
    "        batch[\"ins\"] = batch[\"text\"] + \" You are classifying a news article, Choose one of the four categories, World, Business, Sport, and Tech. Answer is \"\n",
    "    elif dataset == \"sst2\":\n",
    "        batch[\"ins\"] = batch[\"sentence\"] + \" In a sentiment classification task between positive and negative choices, the sentiment of this sentence is \"\n",
    "    elif dataset == \"imdb\":\n",
    "        batch[\"ins\"] = batch[\"text\"] + \" Based on this opinion, decide what the sentiment is, choose between positive and negative. Answer is \"\n",
    "    return batch\n",
    "\n",
    "instruction_data = shuffled_data.map(instruction, batched=False)\n",
    "\n",
    "def gpt_label(d):\n",
    "    if dataset == \"news\":\n",
    "        if d[\"label\"] == 0:\n",
    "            d[\"gpt_label\"] = world_id\n",
    "        elif d[\"label\"] == 1:\n",
    "            d[\"gpt_label\"] = sport_id\n",
    "\n",
    "        elif d[\"label\"] == 2:\n",
    "            d[\"gpt_label\"] = business_id\n",
    "        elif d[\"label\"] == 3:\n",
    "            d[\"gpt_label\"] = tech_id\n",
    "            \n",
    "    elif dataset == \"sst2\" or dataset == \"imdb\":\n",
    "        if d[\"label\"] == 0:\n",
    "            d[\"gpt_label\"] = negative_id\n",
    "        elif d[\"label\"] == 1:\n",
    "            d[\"gpt_label\"] = positive_id\n",
    "    return d\n",
    "\n",
    "\n",
    "instruction_data = instruction_data.map(gpt_label, batched=False)\n",
    "\n",
    "instruction_data = instruction_data.remove_columns([\"label\"])\n",
    "\n",
    "if dataset == \"sst2\":\n",
    "    max_length = 32\n",
    "    if model_id == \"gemma-7b\":\n",
    "        batch_size = 8\n",
    "    elif model_id == \"gemma-2b\":\n",
    "        batch_size = 16\n",
    "\n",
    "elif dataset == \"news\":\n",
    "    max_length = 64\n",
    "    if model_id == \"gemma-7b\":\n",
    "        batch_size = 6\n",
    "    elif model_id == \"gemma-2b\":\n",
    "        batch_size = 12\n",
    "    instruction_data[\"test\"] = instruction_data[\"test\"].select(range(3000))\n",
    "\n",
    "elif dataset == \"imdb\":\n",
    "    max_length = 128\n",
    "    if model_id == \"gemma-7b\":\n",
    "        batch_size = 3\n",
    "    elif model_id == \"gemma-2b\":\n",
    "        batch_size = 12\n",
    "    instruction_data[\"test\"] = instruction_data[\"test\"].select(range(3000))\n",
    "def func2(a):\n",
    "    a = tokenizer(a['ins'], padding=\"max_length\",max_length = max_length)\n",
    "    return a\n",
    "\n",
    "instruction_data = instruction_data.map(func2, batched= True, batch_size = batch_size)\n",
    "instruction_data = instruction_data.filter(lambda x: len(x[\"input_ids\"]) <= max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5723/5723 [22:49<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss: 0.03795564974565765\n",
      "epoch 0 train acc: 0.8789129912401424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:02<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 eval loss: 0.03171209569043684\n",
      "epoch 0 eval acc: 0.8719512195121951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5723/5723 [22:50<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 train loss: 0.015095048907970242\n",
      "epoch 1 train acc: 0.9530113375712694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:02<00:00,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 eval loss: 0.03298880543031662\n",
      "epoch 1 eval acc: 0.9024390243902439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "set_seed(123)\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "op = torch.optim.Adam(model.parameters(),lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(op,\"max\",factor=0.5, patience = 1,verbose=True)\n",
    "\n",
    "num_epoch = 2\n",
    "model.to(device)\n",
    "import torch.nn as nn\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "from tqdm import tqdm\n",
    "for epoch in range(num_epoch):\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for i in tqdm(range(0, len( instruction_data[\"train\"]),batch_size)):\n",
    "        ids = torch.tensor(instruction_data[\"train\"][i:i+batch_size][\"input_ids\"],dtype=torch.int64).to(device)\n",
    "        mask = torch.tensor(instruction_data[\"train\"][i:i+batch_size][\"attention_mask\"],dtype=torch.int64).to(device)\n",
    "        label = torch.tensor(instruction_data[\"train\"][i:i+batch_size][\"gpt_label\"],dtype=torch.int64).to(device)\n",
    "        logit = model(input_ids = ids,attention_mask = mask).logits[:,-1,:]\n",
    "        l = loss_fn(logit, label)\n",
    "        l.backward()\n",
    "        op.step()\n",
    "        op.zero_grad()\n",
    "        num_correct = num_correct + (torch.argmax(logit,dim=-1) ==label).sum().item()\n",
    "        total = total + label.shape[0]\n",
    "        loss = loss + l.item()\n",
    " \n",
    "\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    print(f\"epoch {epoch} train loss: {loss/len(instruction_data['train'])}\")\n",
    "    print(f\"epoch {epoch} train acc: {num_correct/total}\")\n",
    "\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len( instruction_data[\"test\"]),batch_size)):\n",
    "            ids = torch.tensor(instruction_data[\"test\"][i:i+batch_size][\"input_ids\"],dtype=torch.int64).to(device)\n",
    "            mask = torch.tensor(instruction_data[\"test\"][i:i+batch_size][\"attention_mask\"],dtype=torch.int64).to(device)\n",
    "            label = torch.tensor(instruction_data[\"test\"][i:i+batch_size][\"gpt_label\"],dtype=torch.int64).to(device)\n",
    "            logit = model(input_ids = ids,attention_mask = mask).logits[:,-1,:]\n",
    "            l = loss_fn(logit, label)\n",
    "            num_correct = num_correct + (torch.argmax(logit,dim=-1) ==label).sum().item()\n",
    "            total = total + label.shape[0]\n",
    "            loss = loss + l.item()\n",
    "        \n",
    "    print(f\"epoch {epoch} eval loss: {loss/len(instruction_data['test'])}\")\n",
    "    print(f\"epoch {epoch} eval acc: {num_correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.merge_and_unload()\n",
    "torch.save(model.base_model.model.state_dict(),\n",
    "            f\"predictor_{model_id}_{dataset}_merged.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faith11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
